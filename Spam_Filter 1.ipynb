{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Filter\n",
    "I will be processing the content of the email and converting it into a list of words. Then comparsion would be made between this list and an another list which is named as \"Keyword\" list. This list contains essential words that would trigger the spam filter. This will also be containing words that won't trigger spam filter. \n",
    "Each word in the processed mail will be compared to the keyword list. If the word is present in the keyword list then the index of the keyword will be noted in another vector. This vector would be acting as nodes of the first layer.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import string\n",
    "import pandas as pd \n",
    "import csv\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_set = pd.read_csv('emails.csv')\n",
    "#keyword = pd.read_csv('vocab.txt')\n",
    "data_set = open('emails.csv')\n",
    "# keyword = open('vocab.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = csv.reader(data_set)\n",
    "#word_list = csv.reader(keyword) # I am not using it anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Loading the data andseperating it into X and Y for ease of building.\n",
    "\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "for (a,b) in file:\n",
    "    X.append(a)\n",
    "    Y.append(b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspam_trigger = []\\nfor word in word_list:\\n    print(word)\\n    x = np.squeeze(word)\\n    spam_trigger.append(str(x))  \\nprint(\"done\")    \\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  IT IS OF NO USE (I TRIED IT SVM KERNEL method of SPAM FILTER) but it failed miserably\n",
    "'''\n",
    "spam_trigger = []\n",
    "for word in word_list:\n",
    "    print(word)\n",
    "    x = np.squeeze(word)\n",
    "    spam_trigger.append(str(x))  \n",
    "print(\"done\")    \n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_data_set = np.zeros(shape = (1, len(Y)-1))\n",
    "for i in range(1, len(Y)-1):\n",
    "    Y_data_set[:, i] = Y[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5728)\n"
     ]
    }
   ],
   "source": [
    "print(Y_data_set.shape)\n",
    "Y_model_data = Y_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5729"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5728)\n",
      "(1, 5728)\n"
     ]
    }
   ],
   "source": [
    "print(Y_model_data.shape)\n",
    "print(Y_data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre Processing and data set distribution\n",
    "   It is very important to preprocess data set to remove puctuautions, stop words ( which doesnot contribute to the learning of the neural networks ) and stemming is also important. Data set has been loaded. Now the data will further be divided into training, development and test set. Another list of name keyword will be loaded for the purpose of comparision.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can be alternative for processing an email but now i am using TFIDF AND COUNT VECTORIZER FOR THIS METHOD\n",
    "# I am not using it any more\n",
    "'''\n",
    "\n",
    "def process_mail(message, lower_case = True, stem = True, stop_words = True, gram = 2 ):\n",
    "    if lower_case:\n",
    "        message = message.lower()\n",
    "    words = word_tokenize(message)\n",
    "    words = [w for w in words if len(w) > 2]\n",
    "    if stop_words:\n",
    "        sw = stopwords.words('english')\n",
    "        words = [word for word in words if word not in sw]\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "    return words    \n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "I will write a functions that will initialize the parameters for the model. The function will be used to initialize parameters for a L layer model (General layer model). It will require a vector of units of layer andoutput parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_L_layer(node_vector):\n",
    "       \n",
    "    parameters = {}\n",
    "    L = len(node_vector)            \n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(node_vector[l], node_vector[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros(shape = (node_vector[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "Activation functions like sigmoid activation function and the sigmoid function will be required in the forward propogation and backward propogation. Derivative of these function will also be rquired hence its time to code them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    A = np.maximum(Z, 0)\n",
    "    return A    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(Z):\n",
    "    A = Z * (1-Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward propogation\n",
    "Now that I am done with the initialization process, i am going to implement forward propogation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(parameters, X):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2              \n",
    "        \n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "     \n",
    "        Z = np.dot(parameters[\"W\" + str(l)], A_prev) + parameters[\"b\" + str(l)]\n",
    "        linear_cache = (A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)])\n",
    "        A = relu(Z)\n",
    "    \n",
    "        cache = (linear_cache)\n",
    "        caches.append(cache)\n",
    "        \n",
    "    Z_hat = np.dot(parameters[\"W\" + str(L)], A) + parameters[\"b\" + str(L)]\n",
    "    linear_cache = (A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)])\n",
    "    A_hat = sigmoid(Z_hat)\n",
    "    cache = (linear_cache)\n",
    "    caches.append(cache)\n",
    "    \n",
    "            \n",
    "    return A_hat, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost function\n",
    "\n",
    "Now I will implement backward propagation. I also need to compute the cost to check if my model is actually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A_hat, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = (-1 / m) * np.sum(np.multiply(Y, np.log(A_hat)) + np.multiply(1 - Y, np.log(1 - A_hat)))\n",
    "    cost = np.squeeze(cost)     \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back propogation\n",
    "Now that we have computed cost and forward propogation its time to code back propogation. I am doing it two steps because in one code cell it is becoming messy and difficult to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop_1(dA, cache, AL, Y_hat, activation):\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "  \n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        \n",
    "        dZ = dA * relu_derivative(np.dot(W, A_prev) + b)  \n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)  \n",
    "        db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T, dZ) \n",
    "                \n",
    "    elif activation == \"sigmoid\":\n",
    "        \n",
    "        # dZ = AL - Y_hat     # This line is for linear regression\n",
    "        dZ = dA * sigmoid_derivative(AL) \n",
    "        dW = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        db = (1/m) * np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "            \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propogation(A_hat, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) \n",
    "    m = A_hat.shape[1]\n",
    "    Y = Y.reshape(A_hat.shape) \n",
    "    \n",
    "    dAL = - (np.divide(Y, A_hat) - np.divide(1 - Y, 1 - A_hat))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = backward_prop_1(dAL, current_cache, A_hat, Y, activation = \"sigmoid\")\n",
    "    \n",
    "    # Ranges from l-2 to 0\n",
    "    for l in reversed(range(L-1)):\n",
    "        \n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev_temp, dW_temp, db_temp = backward_prop_1(grads[\"dA\" + str(l+1)], current_cache, A_hat, Y, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "       \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update Parameters\n",
    "Now its time to update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "        \n",
    "    L = len(parameters) // 2 \n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * grads[\"dW\" + str(l + 1)])  \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    for l in range(L):\n",
    "    \n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "   \n",
    "    \n",
    "    L = len(parameters) // 2                 \n",
    "    v_corrected = {}                         \n",
    "    s_corrected = {}                         \n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "       \n",
    "        v[\"dW\" + str(l+1)] = (v[\"dW\" + str(l+1)] * beta1) + ((1-beta1) * grads[\"dW\" + str(l+1)])\n",
    "        v[\"db\" + str(l+1)] = (v[\"db\" + str(l+1)] * beta1) + ((1-beta1) * grads[\"db\" + str(l+1)])\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        \n",
    "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1-np.power(beta1,t)) \n",
    "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1-np.power(beta1,t))\n",
    "        \n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        \n",
    "        s[\"dW\" + str(l+1)] = (s[\"dW\" + str(l+1)] * beta2) + ((1-beta2) * (grads[\"dW\" + str(l+1)] ** 2))\n",
    "        s[\"db\" + str(l+1)] = (s[\"db\" + str(l+1)] * beta2) + ((1-beta2) * (grads[\"db\" + str(l+1)] ** 2))\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        \n",
    "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1-np.power(beta2,t))\n",
    "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1-np.power(beta2,t))\n",
    "        \n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        \n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - (learning_rate * (v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon))) \n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - (learning_rate * (v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)))\n",
    "        \n",
    "\n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Now that we have trained all the required supplements. Here comes the real model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  \\n            \\n            if k < (mini_batches-1):\\n                X = X_data[:, k*mini_batch_size : (k+1)*mini_batch_size]\\n                Y = Y_data[:, k*mini_batch_size : (k+1)*mini_batch_size]\\n            elif k >= mini_batches-1:\\n                X = X_data[:, (k+1)*mini_batch_size :]\\n                Y = Y_data[:, (k+1)*mini_batch_size :]\\n\\n            # Forward propagation\\n\\n            A_hat, caches = forward_propogation(parameters, X)\\n\\n            # Compute cost.\\n\\n            cost = compute_cost(A_hat, Y)\\n\\n            # Backward propagation.\\n\\n            grads = back_propogation(A_hat, Y, caches)\\n\\n\\n            # Update parameters.\\n\\n            #parameters = update_parameters(parameters, grads, learning_rate)\\n            t = t + 1 # Adam counter\\n            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\\n                                                                   t, learning_rate, beta1, beta2,  epsilon)\\n\\n\\n        # Print the cost every 100 training example\\n        if print_cost and i % 100 == 0:\\n            print (\"Cost after iteration %i: %f\" %(i, cost))\\n        if print_cost and i % 100 == 0:\\n            costs.append(cost)\\n\\n    # plot the cost\\n    plt.plot(np.squeeze(costs))\\n    plt.ylabel(\\'cost\\')\\n    plt.xlabel(\\'iterations (per hundreds)\\')\\n    plt.title(\"Learning rate =\" + str(learning_rate))\\n    plt.show()\\n    \\n    return parameters, A_hat\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Spam_model(X, Y, node_vector,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, learning_rate = 0.009, num_iterations = 2500, print_cost=True):\n",
    "   \n",
    "    costs = []\n",
    "    t = 0\n",
    "    \n",
    "        # Parameters initialization.\n",
    "    \n",
    "    parameters = initialize_parameters_L_layer(node_vector)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "    \n",
    "        A_hat, caches = forward_propogation(parameters, X)\n",
    "              \n",
    "        # Compute cost.\n",
    "      \n",
    "        cost = compute_cost(A_hat, Y)\n",
    "          \n",
    "        # Backward propagation.\n",
    "      \n",
    "        grads = back_propogation(A_hat, Y, caches)\n",
    "      \n",
    " \n",
    "        # Update parameters.\n",
    "      \n",
    "        #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        t = t + 1 # Adam counter\n",
    "        parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "                        \n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters, A_hat\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "##_________________________Model for mini batch gradient descent ( Massive dataset )______________________________________\n",
    "\n",
    "def nn_model(X, Y, mini_batch_size , node_vector,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, learning_rate = 0.0075, num_iterations = 1000, print_cost=True):\n",
    "    \n",
    "    costs = []\n",
    "    t = 0\n",
    "    mini_batch_size = mini_batch_size\n",
    "    X_data = X\n",
    "    Y_data = Y\n",
    "        # Parameters initialization.\n",
    "    \n",
    "    parameters = initialize_parameters_L_layer(node_vector)\n",
    "    v, s = initialize_adam(parameters)\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    print(m)\n",
    "    mini_batches = math.floor(m/mini_batch_size) # 4869/128= 38\n",
    "    print(mini_batches)\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        for k in range(mini_batches): \n",
    "'''\n",
    "            \n",
    "            # __________________________IMPORTANT_____________________________\n",
    "            \n",
    "'''\n",
    "               While you are using mini batch gradient descent make sure that you shuffle each mini batch within itself\n",
    "               This si an important practice which should be done. I am not doing it here. \n",
    "               But you can refer Improving Deep Neural Networks/Week2/Optimization+methods.ipynb from deep learning course\n",
    "'''\n",
    "          \n",
    "'''  \n",
    "            \n",
    "            if k < (mini_batches-1):\n",
    "                X = X_data[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "                Y = Y_data[:, k*mini_batch_size : (k+1)*mini_batch_size]\n",
    "            elif k >= mini_batches-1:\n",
    "                X = X_data[:, (k+1)*mini_batch_size :]\n",
    "                Y = Y_data[:, (k+1)*mini_batch_size :]\n",
    "\n",
    "            # Forward propagation\n",
    "\n",
    "            A_hat, caches = forward_propogation(parameters, X)\n",
    "\n",
    "            # Compute cost.\n",
    "\n",
    "            cost = compute_cost(A_hat, Y)\n",
    "\n",
    "            # Backward propagation.\n",
    "\n",
    "            grads = back_propogation(A_hat, Y, caches)\n",
    "\n",
    "\n",
    "            # Update parameters.\n",
    "\n",
    "            #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            t = t + 1 # Adam counter\n",
    "            parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                                   t, learning_rate, beta1, beta2,  epsilon)\n",
    "\n",
    "\n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters, A_hat\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Vector & Tfidf Vectorizer\n",
    "These are two ways of converting an email content into the numerical data. these are just one liner code to apply STEMMING, AVOID STOP WORDS, TOKENIZE and CREATE DATA TO NUMBERS.\n",
    "\n",
    "Countvectorizer works the same way as tfidf vector, What you have to do is just replace tfidf with CountVectorizer() only. Rest remains the same. Tfidf works better in large data set so i have used it here. I also tried to do it myself. It was actualy what in learnt in machine learning course to develop spam filter in octave using SVM Kernel features. BU tut went wrong and was not working properly because my cost graph eventually stopped at 0.59 at didnot decreased from it any further. So I am using them now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cv = TfidfVectorizer(min_df = 1, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef Myway_spam_inputvec():\\n    \\n    # This is what i did (for SVM KERNEL method) but it is not working properly here.\\n    \\n    X_data_set = np.random.randn(len(spam_trigger), len(X)-1)\\n    t = 0\\n    for i in range(len(X)-1):\\n        X_data = []\\n        message = X[i+1]\\n        print(f\"Processing the {i} column of the input layer\")\\n        pure_words = process_mail(message, lower_case = True, stem = True, stop_words = False, gram = 2 )\\n\\n        #print(words)#  Apply it to confirm whether the list is correct or not\\n\\n        for words in spam_trigger:\\n            for email_words in pure_words:\\n                if email_words == words:\\n                    X_data.append(spam_trigger.index(words)) \\n\\n            X_data.append(0)\\n\\n\\n        for k in range(len(spam_trigger)):\\n            X_data_set[k][t] = X_data[k]\\n\\n        t += 1\\n            # A column vector is made of the required X input but switch between train set dev set and test set is still pending.\\n            '"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am NOT USING IT anymore\n",
    "'''\n",
    "def Myway_spam_inputvec():\n",
    "    \n",
    "    # This is what i did (for SVM KERNEL method) but it is not working properly here.\n",
    "    \n",
    "    X_data_set = np.random.randn(len(spam_trigger), len(X)-1)\n",
    "    t = 0\n",
    "    for i in range(len(X)-1):\n",
    "        X_data = []\n",
    "        message = X[i+1]\n",
    "        print(f\"Processing the {i} column of the input layer\")\n",
    "        pure_words = process_mail(message, lower_case = True, stem = True, stop_words = False, gram = 2 )\n",
    "\n",
    "        #print(words)#  Apply it to confirm whether the list is correct or not\n",
    "\n",
    "        for words in spam_trigger:\n",
    "            for email_words in pure_words:\n",
    "                if email_words == words:\n",
    "                    X_data.append(spam_trigger.index(words)) \n",
    "\n",
    "            X_data.append(0)\n",
    "\n",
    "\n",
    "        for k in range(len(spam_trigger)):\n",
    "            X_data_set[k][t] = X_data[k]\n",
    "\n",
    "        t += 1\n",
    "            # A column vector is made of the required X input but switch between train set dev set and test set is still pending.\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_completedata = cv.fit_transform(X[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model_data = X_completedata.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.11913249, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5728, 36996)\n"
     ]
    }
   ],
   "source": [
    "print(X_model_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['100', 'affordability', 'aim', 'automaticaily', 'benefits',\n",
       "        'break', 'budget', 'business', 'catchy', 'change', 'changes',\n",
       "        'ciear', 'clear', 'collaboration', 'company', 'content',\n",
       "        'convenience', 'corporate', 'creativeness', 'days', 'distinctive',\n",
       "        'drafts', 'easier', 'easy', 'effective', 'efforts', 'extra',\n",
       "        'fees', 'formats', 'gaps', 'good', 'guaranteed', 'hand', 'hard',\n",
       "        'havinq', 'hotat', 'identity', 'ieader', 'image', 'information',\n",
       "        'interested', 'iogo', 'irresistible', 'isguite', 'isoverwhelminq',\n",
       "        'letsyou', 'list', 'logo', 'logos', 'look', 'love', 'lt', 'make',\n",
       "        'management', 'market', 'marketing', 'naturally', 'nowadays',\n",
       "        'ordered', 'organization', 'original', 'outstanding', 'portfolio',\n",
       "        'practicable', 'products', 'promise', 'promptness', 'provide',\n",
       "        'provided', 'really', 'recollect', 'reflect', 'result',\n",
       "        'satisfaction', 'shouldn', 'specially', 'stationery', 'statlonery',\n",
       "        'structure', 'stylish', 'subject', 'suqgestions', 'surethat',\n",
       "        'task', 'unlimited', 'use', 'website', 'world'], dtype='<U24')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of selected words by TFIDF Vectorizer of the first text email of the text array(X)\n",
    "\n",
    "cv.inverse_transform(X_model_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef random_shuffle_two_vec(X, Y):\\n    a = X\\n    b = Y\\n\\n    c = list(zip(a, b))\\n\\n    np.random.shuffle(c)\\n\\n    a, b = zip(*c)\\n\\n    return a, b\\n    '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May be used but not using it here\n",
    "# I am not using it here\n",
    "'''\n",
    "def random_shuffle_two_vec(X, Y):\n",
    "    a = X\n",
    "    b = Y\n",
    "\n",
    "    c = list(zip(a, b))\n",
    "\n",
    "    np.random.shuffle(c)\n",
    "\n",
    "    a, b = zip(*c)\n",
    "\n",
    "    return a, b\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model_data_actual = X_model_data.T\n",
    "\n",
    "train_set_X = X_model_data_actual[:, :4869]\n",
    "\n",
    "X_test = X_model_data_actual[:, 4869:]\n",
    "Y_train = Y_model_data[:, :4869]\n",
    "\n",
    "Y_test = Y_model_data[:, 4869:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36996, 4869)\n",
      "(36996, 859)\n",
      "(1, 4869)\n"
     ]
    }
   ],
   "source": [
    "print(train_set_X.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.165143\n",
      "Cost after iteration 200: 0.093161\n",
      "Cost after iteration 300: 0.058439\n",
      "Cost after iteration 400: 0.039975\n",
      "Cost after iteration 500: 0.029186\n",
      "Cost after iteration 600: 0.022370\n",
      "Cost after iteration 700: 0.017792\n",
      "Cost after iteration 800: 0.014564\n",
      "Cost after iteration 900: 0.012200\n",
      "Cost after iteration 1000: 0.010414\n",
      "Cost after iteration 1100: 0.009030\n",
      "Cost after iteration 1200: 0.007936\n",
      "Cost after iteration 1300: 0.007054\n",
      "Cost after iteration 1400: 0.006334\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters, A_hat = Spam_model(train_set_X, Y_train, node_vector = [36996, 20, 5, 1], num_iterations = 1500, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_test, cache = forward_propogation(parameters, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_pred = np.round(A_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Prediction\n",
    "\n",
    "A_hat_train, cache = forward_propogation(parameters, train_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_hat_train_pred = np.round(A_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_hat_train_pred[:, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hallahu = A_hat_train_pred - Y_train\n",
    "print(hallahu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 0., 0., ..., 0., 0., 0.])]\n"
     ]
    }
   ],
   "source": [
    "pred_vec=[]\n",
    "for x in hallahu:\n",
    "    pred_vec.append(x)\n",
    "print(pred_vec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4868\n"
     ]
    }
   ],
   "source": [
    "k = 0\n",
    "for assum in pred_vec:\n",
    "    for numb in assum:\n",
    "        if numb == 0:\n",
    "            k += 1\n",
    "print(k)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set accuracy is 0.9997946190182789\n"
     ]
    }
   ],
   "source": [
    "l = 4868/4869\n",
    "print(f\"Train set accuracy is {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta= np.squeeze(A_hat_pred - Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "t=0\n",
    "for k in delta:\n",
    "    if k != 0:\n",
    "        t=t+1\n",
    "        \n",
    "print(t)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy is 0.9988358556461001\n"
     ]
    }
   ],
   "source": [
    "cal = (859-1)/859\n",
    "print(f\"Test set accuracy is {cal}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
